# search.py
"""
æ–‡ç« æœç´¢å’Œè¿‡æ»¤æ¨¡å—
"""

import json
import logging
import re
import yaml
import requests
import feedparser
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
from dateutil import parser as date_parser

from .ai_filter import filter_articles_with_ai
from .feishu_pusher import push_to_feishu

logger = logging.getLogger(__name__)

# PubMed E-utilities API
PUBMED_BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
ESEARCH_URL = PUBMED_BASE_URL + "esearch.fcgi"
EFETCH_URL = PUBMED_BASE_URL + "efetch.fcgi"


class ArticleSearcher:
    """
    ç”Ÿç‰©æ–‡ç« æœç´¢å™¨

    æ”¯æŒä»PubMedå’ŒBioRxivæœç´¢æ–‡ç« ï¼Œå¹¶è¿›è¡Œå…³é”®è¯å’Œä½œè€…è¿‡æ»¤ã€‚
    """

    def __init__(self, config_file: str = "article_search_config.yaml",
                 secrets_file: str = "secrets.yaml"):
        """
        åˆå§‹åŒ–æ–‡ç« æœç´¢å™¨

        Args:
            config_file: ä¸»é…ç½®æ–‡ä»¶è·¯å¾„
            secrets_file: æ•æ„Ÿä¿¡æ¯é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_file, secrets_file)
        self._setup_logging()

    def _load_config(self, config_file: str, secrets_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        # åŠ è½½ä¸»é…ç½®
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… ä¸»é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
        except FileNotFoundError:
            logger.warning(f"âš ï¸ ä¸»é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            config = self._get_default_config()

        # åŠ è½½æ•æ„Ÿä¿¡æ¯é…ç½®
        secrets = {}
        if Path(secrets_file).exists():
            try:
                with open(secrets_file, 'r', encoding='utf-8') as f:
                    secrets = yaml.safe_load(f) or {}
                logger.info(f"âœ… æ•æ„Ÿä¿¡æ¯é…ç½®åŠ è½½æˆåŠŸ: {secrets_file}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ•æ„Ÿä¿¡æ¯é…ç½®åŠ è½½å¤±è´¥: {e}")

        # åˆå¹¶é…ç½®
        merged_config = self._merge_configs(config, secrets)

        # è§£æå˜é‡å¼•ç”¨
        merged_config = self._resolve_variable_references(merged_config, merged_config)

        return merged_config

    def _merge_configs(self, main_config: Dict, secrets_config: Dict) -> Dict:
        """åˆå¹¶ä¸»é…ç½®å’Œæ•æ„Ÿä¿¡æ¯é…ç½®"""
        import copy
        merged = copy.deepcopy(main_config)
        merged['secrets'] = secrets_config
        return merged

    def _resolve_variable_references(self, config: Any, root_config: Dict, max_depth: int = 10) -> Any:
        """è§£æé…ç½®ä¸­çš„å˜é‡å¼•ç”¨"""
        if max_depth <= 0:
            raise ValueError("é…ç½®å˜é‡å¼•ç”¨æ·±åº¦è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯å¼•ç”¨")

        if isinstance(config, dict):
            resolved = {}
            for key, value in config.items():
                resolved[key] = self._resolve_variable_references(value, root_config, max_depth - 1)
            return resolved
        elif isinstance(config, list):
            return [self._resolve_variable_references(item, root_config, max_depth - 1) for item in config]
        elif isinstance(config, str) and config.startswith("${") and config.endswith("}"):
            # è§£æå˜é‡å¼•ç”¨
            var_path = config[2:-1]
            try:
                return self._get_nested_value(root_config, var_path)
            except KeyError:
                logger.warning(f"âš ï¸ æ— æ³•è§£æå˜é‡å¼•ç”¨: {config}ï¼Œä¿ç•™åŸå€¼")
                return config
        else:
            return config

    def _get_nested_value(self, config: Dict, path: str) -> Any:
        """ä»åµŒå¥—å­—å…¸ä¸­è·å–å€¼"""
        keys = path.split(".")
        current = config

        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                raise KeyError(f"è·¯å¾„ {path} ä¸­çš„é”® {key} ä¸å­˜åœ¨")

        return current

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "search_config": {
                "days": 7,
                "max_results_per_journal": 20
            },
            "journals": {
                "pubmed_journals": ["nature", "science", "genome_biology"],
                "biorxiv": {
                    "enabled": True,
                    "subjects": ["bioinformatics", "computational_biology"]
                }
            },
            "keywords": {
                "any": ["cancer", "tumor", "DNA", "RNA"],
                "all": []
            },
            "authors": {
                "include": [],
                "exclude": []
            },
            "output": {
                "filename_format": "search_results_{days}days.json",
                "show_details": True,
                "abstract_max_length": -1
            }
        }

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )

    def search_articles(self, days: int = 7, max_results_per_journal: int = 20) -> Dict[str, List[Dict]]:
        """
        æœç´¢æ–‡ç« 

        Args:
            days: æœç´¢æœ€è¿‘å‡ å¤©
            max_results_per_journal: æ¯ä¸ªæœŸåˆŠæœ€å¤§ç»“æœæ•°

        Returns:
            Dict[str, List[Dict]]: å„æœŸåˆŠçš„æ–‡ç« åˆ—è¡¨
        """
        # è·å–æœŸåˆŠåˆ—è¡¨
        journals = self.config["journals"]["pubmed_journals"].copy()
        if self.config["journals"]["biorxiv"]["enabled"]:
            journals.append("biorxiv")

        # è·å–å…³é”®è¯
        keywords = self.config["keywords"]["any"] + self.config["keywords"]["all"]

        # BioRxiv å­¦ç§‘
        biorxiv_subjects = self.config["journals"]["biorxiv"]["subjects"]

        # ä½œè€…é…ç½®
        author_config = self.config.get("authors", {})

        logger.info(f"å¼€å§‹æœç´¢æœ€è¿‘ {days} å¤©çš„æ–‡ç« ...")
        results = self._search_journals(journals, keywords, days, max_results_per_journal,
                                      biorxiv_subjects, author_config)

        logger.info(f"æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {sum(len(articles) for articles in results.values())} ç¯‡æ–‡ç« ")
        return results

    def _search_journals(self, journals: List[str], keywords: List[str], days: int,
                        max_results_per_journal: int, biorxiv_subjects: List[str],
                        author_config: Dict) -> Dict[str, List[Dict]]:
        """ä»æŒ‡å®šæœŸåˆŠæœç´¢æ–‡ç« """
        results = {}

        for journal in journals:
            journal_lower = journal.lower()

            logger.info(f"æœç´¢æœŸåˆŠ: {journal}")

            if journal_lower == "biorxiv":
                articles = self._search_biorxiv(biorxiv_subjects, keywords, days, max_results_per_journal)
                # å¯¹BioRxivæ–‡ç« è¿›è¡Œä½œè€…è¿‡æ»¤
                articles = self._filter_by_authors(articles, author_config, source="biorxiv")
            else:
                # ç›´æ¥ä½¿ç”¨æœŸåˆŠåç§°åŠ ä¸Š[Journal]åç¼€ä½œä¸ºPubMedæŸ¥è¯¢
                journal_query = f"{journal}[Journal]"
                articles = self._search_pubmed_journal(journal_query, keywords, days, max_results_per_journal)
                # PubMedæ–‡ç« æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦è¿›è¡Œä½œè€…è¿‡æ»¤
                if author_config.get("mode") == "all":
                    articles = self._filter_by_authors(articles, author_config, source="pubmed")

            results[journal] = articles
            logger.info(f"{journal}: æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        return results

    def _search_pubmed_journal(self, journal_query: str, keywords: List[str],
                              days: int, max_results: int) -> List[Dict]:
        """ä»PubMedæŒ‡å®šæœŸåˆŠæœç´¢æ–‡ç« """
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        date_range = f"{start_date.strftime('%Y/%m/%d')}:{end_date.strftime('%Y/%m/%d')}[dp]"

        # æ„å»ºæŸ¥è¯¢
        query_parts = [journal_query, date_range]
        if keywords:
            keyword_query = " OR ".join(f'"{kw}"' for kw in keywords)
            query_parts.append(f"({keyword_query})")

        query = " AND ".join(f"({part})" for part in query_parts)

        # æœç´¢
        search_params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "retmode": "json",
            "sort": "relevance"
        }

        try:
            response = requests.get(ESEARCH_URL, params=search_params, timeout=30)
            response.raise_for_status()
            search_results = response.json()

            pmids = search_results["esearchresult"]["idlist"]
            if not pmids:
                return []

            # è·å–æ–‡ç« è¯¦æƒ…
            fetch_params = {
                "db": "pubmed",
                "id": ",".join(pmids),
                "retmode": "xml"
            }

            fetch_response = requests.get(EFETCH_URL, params=fetch_params, timeout=30)
            fetch_response.raise_for_status()

            # è§£æXMLç»“æœ
            articles = self._parse_pubmed_xml(fetch_response.text)
            return articles[:max_results]

        except Exception as e:
            logger.error(f"æœç´¢PubMedå¤±è´¥: {e}")
            return []

    def _parse_pubmed_xml(self, xml_content: str) -> List[Dict]:
        """è§£æPubMed XMLç»“æœ"""
        import xml.etree.ElementTree as ET

        articles = []
        root = ET.fromstring(xml_content)

        for article in root.findall(".//PubmedArticle"):
            try:
                # æå–æ ‡é¢˜
                title_elem = article.find(".//ArticleTitle")
                title = title_elem.text if title_elem is not None else "No title"

                # æå–æ‘˜è¦
                abstract_elem = article.find(".//AbstractText")
                abstract = abstract_elem.text if abstract_elem is not None else ""

                # æå–ä½œè€…
                authors = []
                for author_elem in article.findall(".//Author"):
                    last_name = author_elem.find("LastName")
                    fore_name = author_elem.find("ForeName")
                    if last_name is not None and fore_name is not None:
                        # PubMedæ ¼å¼: "LastName, ForeName"
                        # è½¬æ¢ä¸ºæ›´å¸¸è§çš„ "ForeName LastName" æ ¼å¼
                        authors.append(f"{fore_name.text} {last_name.text}")
                    elif last_name is not None:
                        authors.append(last_name.text)

                # æå–æœŸåˆŠä¿¡æ¯
                journal_elem = article.find(".//Journal/Title")
                journal = journal_elem.text if journal_elem is not None else "Unknown"

                # æå–DOI
                doi_elem = article.find(".//ELocationID[@EIdType='doi']")
                doi = doi_elem.text if doi_elem is not None else ""

                # æå–å‘è¡¨æ—¥æœŸ
                pub_date_elem = article.find(".//PubDate")
                published = "Unknown"
                if pub_date_elem is not None:
                    year = pub_date_elem.find("Year")
                    month = pub_date_elem.find("Month")
                    day = pub_date_elem.find("Day")
                    if year is not None:
                        date_str = year.text
                        if month is not None:
                            date_str += f"-{month.text.zfill(2)}"
                            if day is not None:
                                date_str += f"-{day.text.zfill(2)}"
                        try:
                            published = datetime.strptime(date_str, "%Y-%m-%d").isoformat()
                        except:
                            published = date_str

                articles.append({
                    'title': title,
                    'abstract': abstract,
                    'authors': authors,
                    'journal': journal,
                    'doi': doi,
                    'published': published,
                    'link': f"https://doi.org/{doi}" if doi else "",
                    'source': 'PubMed'
                })

            except Exception as e:
                logger.error(f"è§£æPubMedæ–‡ç« å¤±è´¥: {e}")
                continue

        return articles

    def _search_biorxiv(self, subjects: List[str], keywords: List[str],
                       days: int, max_results: int) -> List[Dict]:
        """ä»BioRxivæœç´¢æ–‡ç« """
        all_articles = []

        for subject in subjects:
            logger.info(f"è·å–RSS: https://connect.biorxiv.org/biorxiv_xml.php?subject={subject}")

            try:
                # è®¡ç®—æ—¥æœŸèŒƒå›´
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)

                # æ„å»ºRSS URL
                rss_url = f"https://connect.biorxiv.org/biorxiv_xml.php?subject={subject}"

                # è§£æRSS
                feed = feedparser.parse(rss_url)

                for entry in feed.entries:
                    try:
                        # è§£æå‘è¡¨æ—¥æœŸ - BioRxivå¯èƒ½ä½¿ç”¨ä¸åŒçš„æ—¥æœŸå­—æ®µ
                        published_str = entry.get('published', '') or entry.get('updated', '') or entry.get('prism_publicationdate', '')
                        
                        if not published_str:
                            # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œè·³è¿‡æ­¤æ¡ç›®
                            continue
                        
                        # å°è¯•è§£ææ—¥æœŸ
                        try:
                            published_dt = date_parser.parse(published_str)
                        except (ValueError, TypeError):
                            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                import time
                                published_dt = datetime(*entry.published_parsed[:6])
                            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                                import time
                                published_dt = datetime(*entry.updated_parsed[:6])
                            else:
                                # æ— æ³•è§£ææ—¥æœŸï¼Œè·³è¿‡
                                continue
                        
                        if published_dt < start_date:
                            continue

                        # è§£æä½œè€…
                        author_str = entry.get('author', '')
                        authors = self._parse_biorxiv_authors(author_str)

                        article = {
                            'title': entry.get('title', ''),
                            'abstract': entry.get('summary', ''),
                            'authors': authors,
                            'journal': 'BioRxiv',
                            'published': published_dt.isoformat(),
                            'link': entry.get('link', ''),
                            'source': 'BioRxiv',
                        }

                        all_articles.append(article)

                    except Exception as e:
                        # é™é»˜è·³è¿‡è§£æå¤±è´¥çš„æ¡ç›®ï¼Œé¿å…å¤§é‡é”™è¯¯æ—¥å¿—
                        continue

                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                import time
                time.sleep(1)

            except Exception as e:
                logger.error(f"è·å–RSSå¤±è´¥: {e}")
                continue

        # å…³é”®è¯è¿‡æ»¤
        filtered = self._filter_by_keywords(all_articles, keywords)
        return filtered[:max_results]

    def _parse_biorxiv_authors(self, author_str: str) -> List[str]:
        """
        è§£æBioRxivä½œè€…å­—ç¬¦ä¸²ï¼Œä¿ç•™å®Œæ•´çš„ä½œè€…åå­—
        
        BioRxivæ ¼å¼ç¤ºä¾‹:
        - "Theis, Fabian J." -> "Fabian J. Theis"
        - "Baker, David" -> "David Baker"
        - "Regev, Aviv" -> "Aviv Regev"
        """
        if not author_str or not isinstance(author_str, str):
            return []

        # ç§»é™¤å¯èƒ½çš„æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
        author_str = author_str.strip()

        # BioRxivæ ¼å¼: "LastName, FirstName/Initials., LastName, FirstName/Initials."
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥æ­£ç¡®è§£æ
        pattern = r'([^,]+),\s*([^,]+?)(?:\.|,|$)'
        matches = re.findall(pattern, author_str)

        authors = []
        for match in matches:
            last_name = match[0].strip()
            first_name_or_initials = match[1].strip().rstrip('.')
            
            # å°†æ ¼å¼ä» "LastName, FirstName" è½¬æ¢ä¸º "FirstName LastName"
            # è¿™æ ·æ›´ç¬¦åˆå¸¸è§çš„åå­—æ˜¾ç¤ºæ ¼å¼
            if first_name_or_initials:
                # å¦‚æœfirst_name_or_initialsæ˜¯ç¼©å†™ï¼ˆå¦‚ "F. J."ï¼‰ï¼Œä¿ç•™åŸæ ¼å¼
                # å¦‚æœæ˜¯å…¨åï¼ˆå¦‚ "Fabian"ï¼‰ï¼Œè½¬æ¢ä¸º "FirstName LastName"
                if len(first_name_or_initials.split()) == 1 and len(first_name_or_initials) <= 3:
                    # å¯èƒ½æ˜¯å•ä¸ªç¼©å†™ï¼Œä¿æŒ "LastName, Initials" æ ¼å¼
                    authors.append(f"{last_name}, {first_name_or_initials}")
                else:
                    # å…¨åæ ¼å¼ï¼Œè½¬æ¢ä¸º "FirstName LastName"
                    authors.append(f"{first_name_or_initials} {last_name}")
            else:
                # åªæœ‰å§“ï¼Œä¿ç•™åŸæ ¼å¼
                authors.append(last_name)

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ä½œè€…ï¼Œå°è¯•æŒ‰é€—å·åˆ†å‰²
        if not authors:
            parts = [p.strip() for p in author_str.split(',')]
            if len(parts) >= 2:
                # å‡è®¾æ ¼å¼æ˜¯ "LastName, FirstName"
                authors.append(f"{parts[1]} {parts[0]}")
            elif author_str.strip():
                authors.append(author_str.strip())

        return authors

    def _filter_by_keywords(self, articles: List[Dict], keywords: List[str]) -> List[Dict]:
        """æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–‡ç« """
        if not keywords:
            return articles

        filtered = []
        for article in articles:
            title = article.get('title', '').lower()
            abstract = article.get('abstract', '').lower()
            text = title + " " + abstract

            if any(kw.lower() in text for kw in keywords):
                filtered.append(article)

        return filtered

    def _normalize_author_name(self, author_name: str) -> str:
        """
        æ ‡å‡†åŒ–ä½œè€…åå­—ï¼Œç”¨äºåŒ¹é…
        
        å°†ä¸åŒæ ¼å¼çš„åå­—è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ¼å¼è¿›è¡Œæ¯”è¾ƒï¼š
        - "Fabian Theis" -> "fabian theis"
        - "Theis, Fabian" -> "fabian theis"
        - "Theis, Fabian J." -> "fabian theis"
        - "Fabian J. Theis" -> "fabian theis"
        """
        if not author_name:
            return ""
        
        # è½¬æ¢ä¸ºå°å†™å¹¶ç§»é™¤å¤šä½™ç©ºæ ¼
        normalized = author_name.lower().strip()
        
        # å¤„ç† "LastName, FirstName" æ ¼å¼
        if ',' in normalized:
            parts = [p.strip() for p in normalized.split(',')]
            if len(parts) >= 2:
                # äº¤æ¢é¡ºåºï¼šLastName, FirstName -> FirstName LastName
                normalized = f"{parts[1]} {parts[0]}"
            else:
                normalized = parts[0]
        
        # ç§»é™¤ä¸­é—´åå’Œç¼©å†™ï¼ˆä¿ç•™å§“å’Œåï¼‰
        # ä¾‹å¦‚ "fabian j. theis" -> "fabian theis"
        words = normalized.split()
        if len(words) >= 2:
            # ä¿ç•™ç¬¬ä¸€ä¸ªè¯ï¼ˆåï¼‰å’Œæœ€åä¸€ä¸ªè¯ï¼ˆå§“ï¼‰
            # ç§»é™¤ä¸­é—´çš„ç¼©å†™ï¼ˆå¦‚ "j.", "f.", ç­‰ï¼‰
            first_name = words[0]
            last_name = words[-1]
            # å¦‚æœä¸­é—´æœ‰å•å­—æ¯æˆ–ç¼©å†™ï¼Œå¿½ç•¥å®ƒä»¬
            normalized = f"{first_name} {last_name}"
        
        return normalized.strip()
    
    def _author_names_match(self, author1: str, author2: str) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªä½œè€…åå­—æ˜¯å¦æŒ‡å‘åŒä¸€ä¸ªäºº
        
        æ”¯æŒå¤šç§æ ¼å¼åŒ¹é…ï¼š
        - "Fabian Theis" åŒ¹é… "Fabian J. Theis"
        - "Theis, Fabian" åŒ¹é… "Fabian Theis"
        - "Fabian Theis" åŒ¹é… "Theis, Fabian J."
        """
        if not author1 or not author2:
            return False
        
        # æ ‡å‡†åŒ–ä¸¤ä¸ªåå­—
        norm1 = self._normalize_author_name(author1)
        norm2 = self._normalize_author_name(author2)
        
        # å®Œå…¨åŒ¹é…
        if norm1 == norm2:
            return True
        
        # éƒ¨åˆ†åŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å¯¹æ–¹çš„å§“å’Œå
        # ä¾‹å¦‚ "fabian theis" åº”è¯¥åŒ¹é… "fabian j. theis"
        words1_list = norm1.split()
        words2_list = norm2.split()
        
        # å¦‚æœä¸¤ä¸ªåå­—çš„å§“å’Œåéƒ½åŒ¹é…ï¼Œè®¤ä¸ºæ˜¯åŒä¸€ä¸ªäºº
        # è‡³å°‘éœ€è¦åŒ¹é…å§“ï¼ˆæœ€åä¸€ä¸ªè¯ï¼‰å’Œåï¼ˆç¬¬ä¸€ä¸ªè¯ï¼‰
        if len(words1_list) >= 2 and len(words2_list) >= 2:
            # æ£€æŸ¥å§“å’Œåæ˜¯å¦éƒ½åŒ¹é…
            first1 = words1_list[0]
            last1 = words1_list[-1]
            first2 = words2_list[0]
            last2 = words2_list[-1]
            
            # æ£€æŸ¥å§“å’Œåæ˜¯å¦åŒ¹é…
            if (last1 == last2) and (first1 == first2):
                return True
        
        # å¦‚æœæ ‡å‡†åŒ–åçš„åå­—æœ‰åŒ…å«å…³ç³»ï¼Œä¹Ÿè®¤ä¸ºæ˜¯åŒ¹é…
        # ä¾‹å¦‚ "fabian theis" åŒ…å«åœ¨ "fabian j. theis" ä¸­
        if norm1 in norm2 or norm2 in norm1:
            # ä½†éœ€è¦ç¡®ä¿è‡³å°‘åŒ…å«å§“
            words1_list = norm1.split()
            words2_list = norm2.split()
            if words1_list and words2_list:
                if words1_list[-1] == words2_list[-1]:  # å§“åŒ¹é…
                    return True
        
        return False

    def _filter_by_authors(self, articles: List[Dict], author_config: Dict, source: str = "all") -> List[Dict]:
        """
        æ ¹æ®ä½œè€…é…ç½®è¿‡æ»¤æ–‡ç« 
        
        ä½¿ç”¨æ™ºèƒ½åŒ¹é…ç®—æ³•ï¼Œèƒ½å¤Ÿè¯†åˆ«åŒä¸€ä¸ªäººçš„ä¸åŒåå­—æ ¼å¼
        """
        if not author_config or not author_config.get("include"):
            return articles

        mode = author_config.get("mode", "biorxiv_only")
        include_authors = author_config.get("include", [])
        exclude_authors = author_config.get("exclude", [])

        # å¦‚æœæ¨¡å¼æ˜¯biorxiv_onlyä¸”æ¥æºä¸æ˜¯biorxivï¼Œåˆ™ä¸è¿›è¡Œä½œè€…è¿‡æ»¤
        if mode == "biorxiv_only" and source != "biorxiv":
            return articles

        filtered = []
        for article in articles:
            article_authors = article.get("authors", [])
            if not article_authors:
                # å¦‚æœæ–‡ç« æ²¡æœ‰ä½œè€…ä¿¡æ¯ï¼Œæ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä¿ç•™
                if mode == "all" and exclude_authors:
                    # allæ¨¡å¼ä¸‹ï¼Œå¦‚æœæœ‰æ’é™¤åˆ—è¡¨ä¸”æ–‡ç« æ— ä½œè€…ï¼Œä¿ç•™
                    filtered.append(article)
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤çš„ä½œè€…
            if exclude_authors:
                excluded = False
                for excl_author in exclude_authors:
                    for article_author in article_authors:
                        if self._author_names_match(excl_author, article_author):
                            excluded = True
                            break
                    if excluded:
                        break
                if excluded:
                    continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å®šçš„ä½œè€…
            if include_authors:
                author_match = False
                for incl_author in include_authors:
                    for article_author in article_authors:
                        if self._author_names_match(incl_author, article_author):
                            author_match = True
                            break
                    if author_match:
                        break
                if not author_match:
                    continue

            filtered.append(article)

        return filtered

    def filter_with_ai(self, articles: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ä½¿ç”¨AIè¿‡æ»¤æ–‡ç« 

        Args:
            articles: å„æœŸåˆŠçš„æ–‡ç« å­—å…¸

        Returns:
            Dict[str, List[Dict]]: è¿‡æ»¤åçš„æ–‡ç« å­—å…¸
        """
        ai_config = self.config.get("ai_filtering", {})
        if not ai_config.get("enabled", False):
            logger.info("AIè¿‡æ»¤å·²ç¦ç”¨")
            return articles

        logger.info("ğŸ¤– å¼€å§‹AIè¿‡æ»¤...")

        # æ”¶é›†æ‰€æœ‰æ–‡ç« 
        all_articles = []
        for journal_articles in articles.values():
            all_articles.extend(journal_articles)

        # åº”ç”¨æœ€å¤§æ£€ç´¢ä¸Šé™
        max_articles = ai_config.get("max_articles_for_filtering", 0)
        original_count = len(all_articles)
        if max_articles > 0 and len(all_articles) > max_articles:
            logger.info(f"ğŸ“Š æ£€ç´¢åˆ° {original_count} ç¯‡æ–‡ç« ï¼Œåº”ç”¨æœ€å¤§æ£€ç´¢ä¸Šé™ {max_articles} ç¯‡")
            all_articles = all_articles[:max_articles]
            logger.info(f"âœ“ å·²é™åˆ¶ä¸ºå‰ {max_articles} ç¯‡æ–‡ç« è¿›è¡ŒAIè¿‡æ»¤")
        else:
            logger.info(f"ğŸ“Š æ£€ç´¢åˆ° {original_count} ç¯‡æ–‡ç« ï¼Œå…¨éƒ¨è¿›è¡ŒAIè¿‡æ»¤")

        # AIè¿‡æ»¤
        filtered_articles = filter_articles_with_ai(all_articles, self.config)

        # é‡æ–°ç»„ç»‡ç»“æœæŒ‰æœŸåˆŠåˆ†ç»„
        filtered_results = {}
        for article in filtered_articles:
            journal = article.get('journal', 'Unknown')
            if journal not in filtered_results:
                filtered_results[journal] = []
            filtered_results[journal].append(article)

        logger.info(f"âœ… AIè¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {sum(len(articles) for articles in filtered_results.values())} ç¯‡æ–‡ç« ")
        return filtered_results

    def push_to_feishu(self, original_results: Dict[str, List[Dict]],
                      filtered_results: Dict[str, List[Dict]], days: int = 7) -> bool:
        """
        æ¨é€ç»“æœåˆ°é£ä¹¦

        Args:
            original_results: åŸå§‹æœç´¢ç»“æœ
            filtered_results: AIè¿‡æ»¤åçš„ç»“æœ
            days: æœç´¢å¤©æ•°

        Returns:
            bool: æ¨é€æ˜¯å¦æˆåŠŸ
        """
        feishu_config = self.config.get("feishu", {})
        if not feishu_config.get("enabled", False):
            logger.info("é£ä¹¦æ¨é€å·²ç¦ç”¨")
            return True

        logger.info("ğŸ“¨ å¼€å§‹é£ä¹¦æ¨é€...")

        # å°†å­—å…¸ç»“æœè½¬æ¢ä¸ºæ‰å¹³çš„åˆ—è¡¨
        filtered_articles_list = []
        for journal, articles in filtered_results.items():
            filtered_articles_list.extend(articles)

        # æ¨é€
        success = push_to_feishu(original_results, filtered_articles_list, days, self.config)

        if success:
            logger.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
        else:
            logger.error("âŒ é£ä¹¦æ¨é€å¤±è´¥")

        return success

    def save_results(self, results: Dict[str, List[Dict]], days: int = 7) -> str:
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results: æœç´¢ç»“æœ
            days: æœç´¢å¤©æ•°

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        filename_format = self.config["output"]["filename_format"]
        output_file = filename_format.format(days=days)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            raise

        return output_file

    def run_complete_workflow(self, days: int = 7) -> Dict[str, List[Dict]]:
        """
        è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹ï¼šæœç´¢ -> AIè¿‡æ»¤ -> é£ä¹¦æ¨é€ -> ä¿å­˜ç»“æœ

        Args:
            days: æœç´¢æœ€è¿‘å‡ å¤©

        Returns:
            Dict[str, List[Dict]]: æœ€ç»ˆçš„è¿‡æ»¤ç»“æœ
        """
        # æœç´¢
        results = self.search_articles(days=days)

        # AIè¿‡æ»¤
        filtered_results = self.filter_with_ai(results)

        # é£ä¹¦æ¨é€
        self.push_to_feishu(results, filtered_results, days)

        # ä¿å­˜ç»“æœ
        self.save_results(filtered_results, days)

        return filtered_results